# Top Vision Language Papers
This repository collects and categorizes top vision-language papers based on their approaches and applications, with a special focus on the CLIP model. 

## Contents
- [Vision-Language Pre-training](#Vision-Language-Pre-training)
- [Prompt Learning for Vision-Language Models](#Prompt-Learning-for-Vision-Language-Models)
- [Feature Adapters for Vision-Language Models](#Feature_Adapters_for_Vision-Language_Models)
- [Regularization-Based Prompt Learning](#Regularization-Based-Prompt-Learning)
- [Test-Time Adaptation of Vision-Language Models](#Test-Time-Adaptation-of-Vision-Language-Models)
- [CLIP-based Domain Generalization](#CLIP-based_Domain_Generalization)

 
## Vision-Language Pre-training 

 - [ ] Learning Transferable Visual Models From Natural Language Supervision - CLIP (ICML 2021) [[Paper]](https://arxiv.org/pdf/2103.00020)[[Code]](https://github.com/OpenAI/CLIP) 

- [ ] Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision - ALIGN (ICML 2021) [[Paper]](https://arxiv.org/pdf/2102.05918)[Code]

- [ ] MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining (CVPR 2023) [[paper]](https://arxiv.org/pdf/2208.12262)[[Code]](https://github.com/LightDXY/MaskCLIP)


## Prompt Learning for Vision-Language Models

- [ ] Learning to Prompt for Vision-Language Models (IJCV 2022) [[Paper]](https://arxiv.org/pdf/2109.01134)[[Code]](https://github.com/KaiyangZhou/CoOp)

- [ ] Conditional Prompt Learning for Vision-Language Models (CVPR 2022) [[Paper]](https://arxiv.org/pdf/2203.05557)[Code] 

- [ ]  MaPLe: Multi-modal Prompt Learning (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2210.03117)[[Code]](https://github.com/muzairkhattak/multimodal-prompt-learning) 

- [ ] Fine-tuned CLIP Models are Efficient Video Learners (CVPR 2023) [[Paper]](https://arxiv.org/pdf/2212.03640)[[Code]](https://github.com/muzairkhattak/ViFi-CLIP)

- [ ] PLOT: Prompt Learning with Optimal Transport for Vision-Language Models (ICLR 2023) [[Paper]](https://arxiv.org/pdf/2210.01253)[[Code]](https://github.com/CHENGY12/PLOT)

- [ ] Gradient-regulated meta-prompt learning for generalizable vision-language models (ICCV 2023) [[Paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Gradient-Regulated_Meta-Prompt_Learning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf)[Code]

- [ ] Meta-Adapter: An Online Few-shot Learner for Vision-Language Model (NeurIPS 2023) [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/ad48f017e6c3d474caf511208e600459-Paper-Conference.pdf)[[Code]](https://github.com/ArsenalCheng/Meta-Adapter)

- [ ] GalLoP: Learning Global and Local Prompts for Vision-Language Models (ECCV 2024) [[Paper]](https://arxiv.org/pdf/2407.01400?)[[Code]](https://github.com/MarcLafon/gallop)

- [ ] IntCoOp: Interpretability-Aware Vision-Language Prompt Tuning (EMNLP 2024) [[Paper]](https://arxiv.org/pdf/2406.13683)[Code]

- [ ] Adversarial Prompt Tuning for Vision-Language Models (ECCV 2024) [[Paper]](https://arxiv.org/pdf/2311.11261)[[Code]](https://github.com/TreeLLi/APT)

- [ ] AAPL: Adding Attributes to Prompt Learning for Vision-Language Models (CVPR-W 2024) [[Paper]](https://openaccess.thecvf.com/content/CVPR2024W/PV/papers/Kim_AAPL_Adding_Attributes_to_Prompt_Learning_for_Vision-Language_Models_CVPRW_2024_paper.pdf)[[Code]](https://github.com/Gahyeonkim09/AAPL)

- [ ] PromptKD: Unsupervised Prompt Distillation for Vision-Language Models (CVPR 2024) [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_PromptKD_Unsupervised_Prompt_Distillation_for_Vision-Language_Models_CVPR_2024_paper.pdf)[[Code]](https://github.com/zhengli97/PromptKD)

- [ ] TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model (CVPR 2024) [[Paper]](https://arxiv.org/pdf/2311.18231)[[Code]](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning/)

- [ ] DePT: Decoupled Prompt Tuning (CVPR 2024) [[Paper]](https://arxiv.org/pdf/2407.10704)[[Code]](https://github.com/Koorye/DePT)

- [ ] Quantized Prompt for Efficient Generalization of Vision-Language Models (ECCV 2024) [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_DePT_Decoupled_Prompt_Tuning_CVPR_2024_paper.pdf)[[Code]](https://github.com/beyondhtx/QPrompt)

- [ ] Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language Models (ICML 2024) [[Paper]](https://arxiv.org/pdf/2311.17091)[[Code]](https://github.com/zhiheLu/Ensemble_VLM)

- [ ] Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models (ICLR 2024) [[Paper]](https://arxiv.org/pdf/2408.13979)[[Code]](https://github.com/ShyFoo/Nemesis)

- [ ] Prompt Learning with One-Shot Setting based Feature Space Analysis in Vision-and-Language Models (CVPR-W 2024) [[Paper]](https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/papers/Hirohashi_Prompt_Learning_with_One-Shot_Setting_based_Feature_Space_Analysis_in_CVPRW_2024_paper.pdf)[Code]

- [ ] Cascade Prompt Learning for Vision-Language Model Adaptation (ECCV 2025) [[Paper]](https://arxiv.org/pdf/2409.17805)[[Code]](https://github.com/megvii-research/CasPL)

- [ ] AlignCLIP: Enhancing Stable Representations in Vision-Language Pretraining Models through Attention and Prediction Alignment (ICLR 2025) [[Paper]](https://openreview.net/pdf?id=qm46g9Ri15)[[Code]](https://github.com/sarahESL/AlignCLIP)


## Feature Adapters for Vision-Language Models

- [ ] CLIP-Adapter: Better Vision-Language Models with Feature Adapters (IJCV 2024) [[Paper]](https://arxiv.org/pdf/2110.04544)[[Code]](https://github.com/gaopengcuhk/CLIP-Adapter)

- [ ] MMA: Multi-Modal Adapter for Vision-Language Models (CVPR 2024) [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_MMA_Multi-Modal_Adapter_for_Vision-Language_Models_CVPR_2024_paper.pdf)[[Code]](https://github.com/ZjjConan/Multi-Modal-Adapter)

## Regularization-Based Prompt Learning

- [ ] Self-regulating Prompts: Foundational Model Adaptation without Forgetting (ICCV 2023) [[Paper]](https://arxiv.org/pdf/2307.06948)[[Code]](https://github.com/muzairkhattak/PromptSRC)

- [ ] Consistency-guided Prompt Learning for Vision-Language Models (ICLR 2024) [[Paper]](https://arxiv.org/pdf/2306.01195)[[Code]](https://github.com/ShuvenduRoy/CoPrompt)

- [ ] Style-Pro: Style-Guided Prompt Learning for Generalizable Vision-Language Models (WACV 2025) [[Paper]](https://arxiv.org/pdf/2411.16018)[Code]

## Test-Time Adaptation of Vision-Language Models

- [ ] Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models (NeurIPS 2022) [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/5bf2b802e24106064dc547ae9283bb0c-Paper-Conference.pdf)[[Code]](https://github.com/azshue/TPT)

- [ ] Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization (NeurIPS 2023) [[Paper]](https://arxiv.org/pdf/2311.01459)[[Code]](https://github.com/jameelhassan/PromptAlign)

- [ ] SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models (NeurIPS 2024) [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/cdd0640218a27e9e2c0e52e324e25db0-Paper-Conference.pdf) [code]

- [ ] Efficient Test-Time Adaptation of Vision-Language Models (CVPR 2024) [[paper]](https://arxiv.org/pdf/2403.18293) [[code]](https://github.com/kdiAAA/TDA)

- [ ] Visual-Text Cross Alignment: Refining the Similarity Score in Vision-Language Models (ICML 2024) [[paper]](https://arxiv.org/pdf/2406.02915) [[code]](https://github.com/tmlr-group/WCA)

- [ ] BaFTA: Backprop-Free Test-Time Adaptation For Zero-Shot Vision-Language Models (submitted to ICLR 2024) [[paper]](https://arxiv.org/pdf/2406.11309v2) [code]


## CLIP-based Domain Generalization

- [ ] STYLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization (WACV 2024) [[paper]](https://arxiv.org/pdf/2302.09251) [code]

- [ ] AD-CLIP: Adapting Domains in Prompt Space Using CLIP (ICCV-W 2023) [[paper]](https://arxiv.org/pdf/2308.05659) [[code]](https://github.com/mainaksingha01/AD-CLIP)

- [ ] Any-Shift Prompting for Generalization over Distributions (CVPR 2024) [[paper]](https://arxiv.org/pdf/2402.10099) [code]

